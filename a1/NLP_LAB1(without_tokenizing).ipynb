{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUpHB0ism-0X",
        "outputId": "62d1190a-8fe5-4f71-9227-cf8d79859d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Loading Hindi dataset (streaming)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1000 docs... mem=720.5MB\n",
            "Processed 2000 docs... mem=723.1MB\n",
            "Processed 3000 docs... mem=727.5MB\n",
            "Processed 4000 docs... mem=728.8MB\n",
            "Processed 5000 docs... mem=731.9MB\n",
            "\n",
            "âœ… Processing complete!\n",
            "\n",
            "ðŸ“Š FINAL COMPARISON (on 5000 docs)\n",
            "\n",
            "Metric                    Tokenized       Non-Tokenized  \n",
            "-------------------------------------------------------\n",
            "sentences                 17461           17461          \n",
            "words                     304777          286699         \n",
            "characters                1441349         1441349        \n",
            "avg_sentence_length       17.45           16.42          \n",
            "avg_word_length           3.84            4.09           \n",
            "ttr                       0.0893          0.1111         \n",
            "vocab_size                27217           31847          \n",
            "documents                 5000            5000           \n",
            "\n",
            "ðŸ“ Results saved to hindi_corpus_output/statistics_comparison.json\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# NLP Assignment 1 - Comparison\n",
        "# Tokenized vs Non-Tokenized Stats\n",
        "# ===============================\n",
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import psutil, gc\n",
        "\n",
        "# -------------------- Memory Helpers --------------------\n",
        "def get_memory_usage():\n",
        "    try:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "\n",
        "# -------------------- Sentence Splitter --------------------\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', 'à¥¤']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "# -------------------- Word Tokenizer --------------------\n",
        "def word_tokenize(sentence):\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |\n",
        "        (www\\.[^\\s]+) |\n",
        "        (\\w+@\\w+\\.\\w+) |\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |\n",
        "        (\\d+\\.\\d+) |\n",
        "        ([\\u0900-\\u097F]+) |\n",
        "        ([a-zA-Z0-9_-]+) |\n",
        "        ([^\\s])\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "# -------------------- Statistics Collector --------------------\n",
        "class CorpusStatistics:\n",
        "    def __init__(self, use_tokenizer=True):\n",
        "        self.total_sentences = 0\n",
        "        self.total_words = 0\n",
        "        self.total_characters = 0\n",
        "        self.sentence_lengths = []\n",
        "        self.word_lengths = []\n",
        "        self.vocabulary = Counter()\n",
        "        self.processed_documents = 0\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "\n",
        "    def process_document(self, text: str):\n",
        "        if not text or not text.strip():\n",
        "            return\n",
        "        text = text.strip()\n",
        "        if len(text) > 50000:\n",
        "            return  # skip too long\n",
        "\n",
        "        sentences = sentence_split(text)\n",
        "        doc_word_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if not sentence.strip():\n",
        "                continue\n",
        "\n",
        "            if self.use_tokenizer:\n",
        "                words = word_tokenize(sentence)\n",
        "            else:\n",
        "                words = sentence.split()  # simple whitespace split\n",
        "\n",
        "            if words:\n",
        "                self.total_sentences += 1\n",
        "                self.total_words += len(words)\n",
        "                self.total_characters += len(sentence)\n",
        "                self.sentence_lengths.append(len(words))\n",
        "                self.vocabulary.update(words)\n",
        "                for word in words:\n",
        "                    self.word_lengths.append(len(word))\n",
        "                doc_word_count += len(words)\n",
        "\n",
        "        if doc_word_count > 0:\n",
        "            self.processed_documents += 1\n",
        "\n",
        "    def compute_statistics(self):\n",
        "        if self.total_sentences == 0:\n",
        "            return {}\n",
        "        avg_sentence_length = sum(self.sentence_lengths) / len(self.sentence_lengths)\n",
        "        avg_word_length = sum(self.word_lengths) / len(self.word_lengths) if self.word_lengths else 0\n",
        "        unique_tokens = len(self.vocabulary)\n",
        "        total_tokens = self.total_words\n",
        "        ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'sentences': self.total_sentences,\n",
        "            'words': self.total_words,\n",
        "            'characters': self.total_characters,\n",
        "            'avg_sentence_length': round(avg_sentence_length, 2),\n",
        "            'avg_word_length': round(avg_word_length, 2),\n",
        "            'ttr': round(ttr, 4),\n",
        "            'vocab_size': unique_tokens,\n",
        "            'documents': self.processed_documents\n",
        "        }\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "def main():\n",
        "    print(\"ðŸ“˜ Loading Hindi dataset (streaming)...\")\n",
        "    hindi_dataset = load_dataset(\n",
        "        \"text\",\n",
        "        data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "        split=\"train\",\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    # Two corpus statistics collectors\n",
        "    tokenized_stats = CorpusStatistics(use_tokenizer=True)\n",
        "    raw_stats = CorpusStatistics(use_tokenizer=False)\n",
        "\n",
        "    max_docs = 5000   # limit for quick test in Colab\n",
        "    processed = 0\n",
        "\n",
        "    for i, example in enumerate(hindi_dataset):\n",
        "        if 'text' not in example or not example['text'].strip():\n",
        "            continue\n",
        "        text = example['text']\n",
        "\n",
        "        tokenized_stats.process_document(text)\n",
        "        raw_stats.process_document(text)\n",
        "        processed += 1\n",
        "\n",
        "        if processed % 1000 == 0:\n",
        "            print(f\"Processed {processed} docs... mem={get_memory_usage():.1f}MB\")\n",
        "\n",
        "        if processed >= max_docs:\n",
        "            break\n",
        "\n",
        "    print(\"\\nâœ… Processing complete!\")\n",
        "    tok_res = tokenized_stats.compute_statistics()\n",
        "    raw_res = raw_stats.compute_statistics()\n",
        "\n",
        "    # Save JSON comparison\n",
        "    os.makedirs(\"hindi_corpus_output\", exist_ok=True)\n",
        "    with open(\"hindi_corpus_output/statistics_comparison.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"tokenized\": tok_res, \"raw\": raw_res}, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Print side-by-side comparison\n",
        "    print(\"\\nðŸ“Š FINAL COMPARISON (on\", processed, \"docs)\\n\")\n",
        "    print(f\"{'Metric':<25} {'Tokenized':<15} {'Non-Tokenized':<15}\")\n",
        "    print(\"-\"*55)\n",
        "    for k in tok_res.keys():\n",
        "        print(f\"{k:<25} {tok_res[k]:<15} {raw_res[k]:<15}\")\n",
        "\n",
        "    print(\"\\nðŸ“ Results saved to hindi_corpus_output/statistics_comparison.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgoZoeb1nAK6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}